{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.test.utils import datapath\n",
    "from pprint import pprint\n",
    "import re\n",
    "from stemming.porter2 import stem\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(content):\n",
    "\tword_tokens = re.sub(r'[!@#$%^&*()_+{}|:\"<>?,./;\\'[\\]\\-=]+', ' ', content).lower().split()\n",
    "\treturn word_tokens\n",
    "\n",
    "def stop_words(word_tokens, stop_word_read):\n",
    "\tstop_word_tokens = tokenization(stop_word_read)\n",
    "\tfilter_stop = [w for w in word_tokens if w not in stop_word_tokens]\n",
    "\treturn filter_stop\n",
    "\n",
    "def porter_stemmer(filter_stop):\n",
    "\tfilter_stemmer = [stem(tokens) for tokens in filter_stop]\n",
    "\treturn filter_stemmer\n",
    "\n",
    "def pre_processing(content, stop_word_read):\n",
    "\tword_tokens = tokenization(content)\n",
    "\tfilter_stop = stop_words(word_tokens, stop_word_read)\n",
    "\tfilter_stemmer = porter_stemmer(filter_stop)\n",
    "\treturn filter_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_model(lda, common_dictionary, corpus_list, num_corpus):\n",
    "\t#Since tuple doesn't support writing operation, we use list so that can write inside values\n",
    "\toverall_score = [[0,0],[1,0],[2,0],[3,0],[4,0],[5,0],[6,0],[7,0],[8,0],[9,0],[10,0],[11,0],[12,0], \\\n",
    "\t\t\t\t\t\t[13,0], [14,0], [15,0], [16,0], [17,0], [18,0], [19,0]]\n",
    "\t#for corpus we chose to calculate\n",
    "\tcorpus = [common_dictionary.doc2bow(text) for text in corpus_list]\n",
    "\n",
    "\tfor doc in corpus:\n",
    "\t\ttopic_scores = lda.get_document_topics(doc, minimum_probability=0.00)\n",
    "\t\tfor i in range (0,20):\n",
    "\t\t\tif overall_score[i][0]==topic_scores[i][0]:\n",
    "\t\t\t\toverall_score[i][1]+=topic_scores[i][1]\n",
    "\n",
    "\t#in the format [(topicID, average_in_corpus)]\n",
    "\taverage_score = [(topic_sum[0],topic_sum[1]/num_corpus) for topic_sum in overall_score]\n",
    "\n",
    "\t#sort the list to get the largest n elements we need\n",
    "\taverage_score = sorted(average_score, key = lambda item: item[1], reverse = True)\n",
    "\n",
    "\n",
    "\tfor i in range (0,10):\n",
    "\t\tprint(\"Topic ID: %d, %.4f\" %(average_score[i][0], average_score[i][1]))\n",
    "\t\tprint(lda.print_topic(average_score[i][0], topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Quran_list = []\n",
    "OT_list = []\n",
    "NT_list = []\n",
    "num_Quran = num_OT = num_NT = 0\n",
    "\n",
    "tokens_list = []\n",
    "\n",
    "#extract the list of stop words\n",
    "with open('englishST.txt', 'r') as f:\n",
    "\tstop_word_read = f.read()\n",
    "\n",
    "with open('train_and_dev.tsv','r') as f:\n",
    "\tfor line in f.readlines():\n",
    "\t\tcorpus = line.split('\\t')[0]\n",
    "\t\tcontent = line.split('\\t')[1]\n",
    "\t\tcontent = pre_processing(content, stop_word_read)\n",
    "\t\ttokens_list+=content\n",
    "\n",
    "\t\tif(corpus == 'Quran'):\n",
    "\t\t\tnum_Quran+=1\n",
    "\t\t\tQuran_list.append(content)\n",
    "\t\t\t#print(re.sub(r'[!@#$%^&*()_+{}|:\"<>?,./;\\'[\\]\\-=]+', ' ', content).lower().split())\n",
    "\t\telif(corpus == 'OT'):\n",
    "\t\t\tnum_OT+=1\n",
    "\t\t\tOT_list.append(content)\n",
    "\t\telif(corpus == 'NT'):\n",
    "\t\t\tnum_NT+=1\n",
    "\t\t\tNT_list.append(content)\n",
    "\n",
    "N = num_Quran + num_OT + num_NT\n",
    "\n",
    "common_texts = Quran_list+OT_list+NT_list\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(common_texts)\n",
    "\n",
    "print(\"length:\", len(common_dictionary))\n",
    "\n",
    "'''\n",
    "This removes all tokens in the dictionary that are:\n",
    "Less frequent than no_below documents (6) and\n",
    "More frequent than no_above documents (fraction of the total corpus size, 0.5).\n",
    "'''\n",
    "common_dictionary.filter_extremes(no_below=5,no_above=0.10)\n",
    "print(\"after filter\",len(common_dictionary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]\n",
    "\n",
    "# Train the model on the corpus.\n",
    "lda = LdaModel(common_corpus, num_topics=20, id2word=common_dictionary)\n",
    "\n",
    "\n",
    "# Save model to disk.\n",
    "temp_file = datapath(\"model\")\n",
    "lda.save(temp_file)\n",
    "\n",
    "# Load a potentially pretrained model from disk.\n",
    "lda = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"corpus1:\")\n",
    "lda_model(lda, common_dictionary, Quran_list, num_Quran)\n",
    "\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"corpus2:\")\n",
    "lda_model(lda, common_dictionary, OT_list, num_OT)\n",
    "\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"corpus3:\")\n",
    "lda_model(lda, common_dictionary, NT_list, num_NT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
